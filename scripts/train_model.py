import os
import sys
# ê²½ë¡œ ì„¤ì • (ëª¨ë¸ í´ë” ì¸ì‹)
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), "..")))
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
from models.generator import Generator
from scripts.dataset.data_preprocessing import BackgroundRemovalDataset

# âœ… í•˜ì´í¼íŒŒë¼ë¯¸í„° ì„¤ì •
BATCH_SIZE = 4
EPOCHS = 10
LEARNING_RATE = 0.0002

# âœ… ë°ì´í„°ì…‹ ë¡œë“œ
dataset = BackgroundRemovalDataset(image_dir="dataset/images", mask_dir="dataset/masks")
train_loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)

# âœ… ëª¨ë¸ ì´ˆê¸°í™”
generator = Generator()
criterion = nn.BCELoss()  # Binary Cross Entropy Loss
optimizer = optim.Adam(generator.parameters(), lr=LEARNING_RATE)

# âœ… ëª¨ë¸ í•™ìŠµ
for epoch in range(EPOCHS):
    total_loss = 0
    for images, masks in train_loader:
        optimizer.zero_grad()

        outputs = generator(images)  # ëª¨ë¸ ì˜ˆì¸¡ ê²°ê³¼
        outputs = torch.mean(outputs, dim=1, keepdim=True)  # ğŸ”¥ 3ì±„ë„ â†’ 1ì±„ë„ ë³€í™˜

        # ğŸ”¥ Sigmoid í™œì„±í™” í•¨ìˆ˜ ì¶”ê°€ (ì¶œë ¥ê°’ì„ 0~1 ë²”ìœ„ë¡œ ì¡°ì •)
        outputs = torch.sigmoid(outputs)

        # ğŸ”¥ ë§ˆìŠ¤í¬ í¬ê¸° ë³€í™˜ (ì¶œë ¥ í¬ê¸°ì™€ ë§ì¶”ê¸°)
        masks = torch.nn.functional.interpolate(masks, size=(512, 512), mode='bilinear', align_corners=False)
        masks = masks / 255.0 if masks.max() > 1 else masks  # ğŸ”¥ ë§ˆìŠ¤í¬ 0~1 ì •ê·œí™”

        loss = criterion(outputs, masks)  # ì •ë‹µ ë§ˆìŠ¤í¬ì™€ ë¹„êµí•˜ì—¬ ì†ì‹¤ ê³„ì‚°
        loss.backward()
        optimizer.step()
        total_loss += loss.item()

    print(f"Epoch [{epoch+1}/{EPOCHS}], Loss: {total_loss / len(train_loader):.4f}")

# âœ… ëª¨ë¸ ì €ì¥
os.makedirs("models", exist_ok=True)
torch.save(generator.state_dict(), "models/generator.pth")
print("âœ… ëª¨ë¸ í•™ìŠµ ì™„ë£Œ ë° ì €ì¥ë¨: models/generator.pth")
